In the signaling simulation, the agents are divided into two groups: the signaling agents and the receiving agents. The signaling agents can signal any action (a number in the range $[0,22]$) to the receiving agents, before deciding on an action to perform. The receiving agents can process transmitted signals from the signaling agents before deciding. There are five different processes performed in each round. In order, they are:

\begin{enumerate}
    \item The signaling agents decide on a signal and transmit it
    \item The receiving agents receive the signal and process it
    \item All agents decide on an action
    \item The agent scores are updated  
    \item The agent beliefs are updated
\end{enumerate}

Even though not mentioned in the itemized list above, the decision and update processes for signaling and receiving agents are different. They do resemble in certain aspects as similar learning algorithms were used for consistency, but the differences are significant enough to warrant a separate explanation, which will be done in the following subsections.

\subsubsection{Signaling Agents}

All signaling agents employ a 2-dimensional beliefs array of the shape (23,23), in which the first dimension represents and holds information for signals, and the second dimension represents and holds information for actions given the signal. In the decision process for all signaling agents, the agent uses the sub-array of shape (23,1), indexed by the signal, to decide on an action. How each order of ToM deals with the signal, decision and update processes are explained in the following sections.

\subsubsubsection{2.2.1.1}{Zero Order Signaling Agent}

The zero order agent employs a simple greedy strategy for signaling and deciding. In both of them, the signal tendency matrix $\mathbf{t}$, which is randomly initialised for each individual agent, is used. It should be noted that while the belief vector $\mathbf{b}$ used by the regular agents as described in Section 2.1.1 represents the agent's belief in what the other agents will do in the next round, the signal tendency matrix $\mathbf{t}$ represents what the agent should do itself to maximise the point gain in the next round: 

\[
\mathbf{t} = [[t_{0_{0}}, \ldots, t_{0_{22}}], \ldots, [t_{22_{0}}, \ldots, t_{22_{22}}]]
\]

where $\mathbf{t_{i_{j}}}$ is the agent's tendency to signal action $\mathbf{i}$ and then perform action $\mathbf{j}$. The \textbf{signal} $\mathbf{\phi}$ and the \textbf{decision} $\mathbf{d_0}$, which is chosen by the agent using the signal $\mathbf{\phi}$ is given by: 

\begin{equation}
\label{eq:zero-order-signal}
\begin{aligned}
    \phi = \arg\max_s \sum_{i=0}^{22} t_{s_i}, \quad i = 0, 1, \ldots, 22. \\
    d_0 = \arg\max_d t_{\phi_d}, \quad d = 0, 1, \ldots, 22.
\end{aligned}
\end{equation}

After this, in the update part, when the agent observes the actions $\mathbf{a_j}$ that have been performed in the previous round, it updates its signal tendencies so that:

\begin{equation}
    t_{i_k} \leftarrow t_{i_k} + LEARNING\_SPEED
\end{equation}

where $k = (a_j + 1) \mod 23$ is the action that would have gained a point from the observed action $\mathbf{a_j}$ in the previous round. This update is performed sequentially for each observed action $\mathbf{a_j}$, and for each signal $0 \leq i \leq 22$. Note, however, that the signal tendencies are not normalized. That is, the order in which updates are performed does not influence the outcome. 
\subsubsubsection{2.2.1.2}{First Order Signaling Agent}

The first order agent employs a similar but slightly different strategy for signaling and deciding. In both of them, the same signal tendency matrix $\mathbf{t}$ is used. The agent also has a zero order agent model $\mathbf{m^0}$, whose \textbf{signal} $\mathbf{\phi}$ (calculations given in \hyperref[eq:zero-order-signal]{Equation 2.1}) is used in the  signaling process. The \textbf{signal} $\mathbf{s^1}$ and the \textbf{decision} $\mathbf{d_1}$ chosen by the agent is given by:

\begin{equation*}
\begin{aligned}
    s^1 = \begin{cases}
        \text{{random\_choice()}}, & \text{{if }} \text{{check\_epsilon()}} \\
        s^0, & \text{{otherwise}}
    \end{cases} \\
    d_1 = \arg\max_d t_{{s^1}_d}, \quad d = 0, 1, \ldots, 22. \qquad
\end{aligned}
\end{equation*}

In the \textbf{update} process, first the zero order agent model $\mathbf{m^0}$ is updated for all observed actions $\mathbf{a_j}$, but with $k = (a_j + 2)$ instead of $(a_j + 1)$ on the right hand side of the equation. This change in k is because the first order agent expects other agents to update their beliefs in response to the observed actions. Then, for all indexes $\mathbf{i}$ and $\mathbf{j}$ in the signal tendency matrix where $0 \leq i, j \leq 22$, the following algorithm is employed for update, where $\mathbf{l_s}$ denotes the LEARNING\_SPEED:

\begin{equation*}
\begin{aligned}
    t_{i_j} = \begin{cases}
        t_{i_j} + l_s \cdot (1 - t_{i_j}), & \text{{if }} i = s^1 \, \& \, j = d_1 \\
        t_{i_j} \cdot (1 - l_s), & \text{{otherwise}}
    \end{cases}
\end{aligned}
\end{equation*}

It's important to note that his has the effect of increasing the tendency $t_{{s^1}_{d_1}}$ of choosing the same signal $s^1$ and making the same decision $d^1$ again in the future.

\subsubsubsection{2.2.1.2}{Second Order Signaling Agent}

The second order signaling agent is a lot like a regular second order agent, with the lower order signaling agent models $\mathbf{m^0}$ and $\mathbf{m^1}$, and the order beliefs $\mathbf{b_o}$. The \textbf{decide} and \textbf{update} processes are the same as the \hyperref[eq:second-order-decide]{Non-signaling Second Order Agent}, with the only difference being the extra \textbf{signal} $\mathbf{s^2}$ chosen by the agent, given by the same algorithm for \textbf{Non-signaling Second Order decide}, but without the modulo operation on the lower order signal $\mathbf{s_l}$. The second order signaling agent is different than the zero and first order signaling agents in the sense that it uses order beliefs $\mathbf{b_o}$ rather than tendencies $\mathbf{t}$, as this and the other second order agents in the study aim to understand which of the lower orders is more dominant in the population and act according to that. The order beliefs allow the second order agent to act as if it were a first order agent, while also keeping the overall consistency of second order agents using order beliefs throughout the study as their results will be compared down the line.

\subsubsection{Receiving Agents}

The receiving agents make use of two different sets of beliefs, one 1-dimensional and one 2-dimensional. The 1-dimensional beliefs array of the shape (23,1) is used to process the incoming signals, and the 2-dimensional beliefs array of the shape (23,23) is used to decide on an action according to the current beliefs formulated through received signals. How each order of ToM deals with the processing the signal, decision and update processes are explained in the following subsections.

\subsubsubsection{2.2.2.1}{Zero Order Receiving Agent}

The zero order receiving agent employs a simple greedy strategy for processing the signal and deciding. In the \textbf{process\_signal} process, the agent uses the the belief vector $\mathbf{b_s}$, where:

\[
\mathbf{b_s} = [b_{0}, b_{1}, \ldots, b_{22}]
\]

on this vector, which is re-initialised every round, the agent performs the same \textbf{update} process as the \hyperref[eq:zero-order-update]{Non-signaling Zero Order Agent}, with the only difference being instead of $\mathbf{b}$, the vector $\mathbf{b_s}$ is used. The vector is re-initialised because it represents the agent's beliefs about the most dominant signal in the current round, and the agent needs to re-evaluate this belief every round. The learning rate for the update operations of this vector is 4 times the simulation-wide learning rate $LEARNING\_SPEED = 0.1$.
For the decide and update processes the connected beliefs vector $\mathbf{b_c}$ is used, where:

\[
\mathbf{b_c} = [[b_{c_{0_{0}}}, b_{c_{0_{1}}}, \ldots, b_{c_{0_{22}}}], \ldots, [b_{c_{22_{0}}}, b_{c_{22_{1}}}, \ldots, b_{c_{22_{22}}}]]
\]

The \textbf{decide} process is given by, where $\psi$ represents the zero order signal belief for the round:

\begin{equation*}
\begin{aligned}
    \psi = \arg\max_s b_{s_i}, \quad i = 0, 1, \ldots, 22. \\
    d_0 = \arg\max_d b_{c_{{\psi}_d}}, \quad d = 0, 1, \ldots, 22.
\end{aligned}
\end{equation*}

The \textbf{update} process is the same as the \hyperref[eq:zero-order-update]{Non-signaling Zero Order Agent}, with the difference being that the connected beliefs vector $\mathbf{b_{c_\psi}}$ is used instead of $\mathbf{b}$, and the following calculation is used:

\begin{equation*}
    b_{c_{\psi_{a_j}}} = b_{c_{\psi_{a_j}}} + LEARNING\_SPEED
\end{equation*}

where $a_j$ represents an action from the actions list for that round.

\subsubsubsection{2.2.2.2}{First Order Receiving Agent}

The first order receiving agent is a lot like the non-signalling first order agent, in the sense that it simply uses a model of a zero order agent to operate, with the sole addition of having a \textbf{process\_signal} process, where the \textbf{process\_signal} process of the zero order agent is called. This agent is essentially a superset of the non-signaling first order agent, with the extra ability to process signals.

\subsubsubsection{2.2.2.3}{Second Order Receiving Agent}

The second order receiving agent is also almost identical the non-signaling second order agent, with only the extra \textbf{process\_signal} process, in which the \textbf{process\_signal} process of the zero and first order agents are called. This agent is also essentially a superset of the non-signaling second order agent, with the extra ability to process signals.

\subsubsection{Simulation}

The signaling simulation will have a total of 11 different initial population configurations to be simulated. These population configurations are chosen specifically because for each ToM order it again investigates the effects listed for the signaling simulation, but also the effects of scenarios in which there are more signaling agents than receiving agents, and vice versa, based on the default order population of the simulation (i.e. the first row of \hyperref[tab:reg-population-table]{Table 2.1}). The ratio of difference for the scenarios with more signaling agents than receiving agents is 30:70 and for the scenarios with more receiving agents it's 70:30. The population configurations are like described below:

\begin{itemize}
    \item Order-wise, the initial 7 scenarios from \hyperref[tab:reg-population-table]{Table 2.1} are exactly the same, with each order being divided into half for signaling and receiving agents.
    \item The other 4 scenarios are with the order configuration of $\{33,33,33\}$ but with the signaling agents being 70\% and 90\% of the population and the receiving agents being 30\% and 10\% of the population, and vice versa.
\end{itemize}


Throughout the simulation, the behavior of the agents will be observed and recorded. This data will include the signals sent by the signaling agents, the actions chosen by all agents, and the payoffs received by each agent. The data will be analyzed, for each simulation individually but also inter-simulation, to determine how the inclusion of signaling in the Mod Game affects the behavior of Theory of Mind agents. The results will be compared to the findings of \cite{de2013much} and \cite{veltman2019training} to evaluate the effect on signaling on emergent agent behaviour.